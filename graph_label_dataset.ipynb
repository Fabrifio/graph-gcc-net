{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset name\n",
    "dataset_name = \"ENZYMES\"\n",
    "\n",
    "# Define datasets destination\n",
    "datasets_folder = \"datasets/\"\n",
    "os.makedirs(datasets_folder, exist_ok = True)\n",
    "\n",
    "# File paths of edge list and node IDs list\n",
    "edges_file = os.path.join(datasets_folder, dataset_name, f\"{dataset_name}.edges\")\n",
    "graph_idx_file = os.path.join(datasets_folder, dataset_name, f\"{dataset_name}.graph_idx\")\n",
    "\n",
    "# Load edge list and node IDs list\n",
    "edges = np.loadtxt(edges_file, dtype=int, delimiter=\",\")\n",
    "graph_idx = np.loadtxt(graph_idx_file, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_tool import all as gt\n",
    "from math import comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep node IDs file and edges file row indexes to read graphs sequentially\n",
    "nodes_file_row = 0\n",
    "edges_file_row = 0\n",
    "\n",
    "# Extract graph ids\n",
    "unique_graph_ids = np.unique(graph_idx)\n",
    "\n",
    "# Global clustering coefficients of graphs in dataset\n",
    "clustering_coefficients = []\n",
    "\n",
    "for current_graph_id in unique_graph_ids:\n",
    "\n",
    "    # Data structure for soring node IDs and edges\n",
    "    current_graph_node_ids = []\n",
    "    current_graph_edges = []\n",
    "\n",
    "    # Extract node IDs of current graph\n",
    "    for idx, graph_id in enumerate(graph_idx[nodes_file_row:], start = nodes_file_row):\n",
    "        if(graph_id == current_graph_id):\n",
    "            current_graph_node_ids.append(idx + 1)\n",
    "        else:\n",
    "            nodes_file_row = nodes_file_row + len(current_graph_node_ids)\n",
    "            break\n",
    "\n",
    "    # Structure to store edges already in the current graph\n",
    "    seen_edges = set()\n",
    "\n",
    "    # Extract edges of current graph\n",
    "    for row, col in edges[edges_file_row:]:\n",
    "        if row in current_graph_node_ids and col in current_graph_node_ids:\n",
    "                \n",
    "            # Sort edge tuple to check if is already present\n",
    "            edge = tuple(sorted((row, col)))        \n",
    "            if(edge not in seen_edges):\n",
    "                current_graph_edges.append((row, col))\n",
    "                seen_edges.add(edge)\n",
    "\n",
    "            # Count at which row we are reading the file\n",
    "            edges_file_row = edges_file_row + 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # Current graph initialization\n",
    "    g = gt.Graph(directed = False)\n",
    "\n",
    "    # Add nodes to graph\n",
    "    for node_id in current_graph_node_ids:\n",
    "        g.add_vertex()\n",
    "\n",
    "    # Map node IDs to range starting from 0 (needed in graphs that are not the first)\n",
    "    node_ids_map = {node_id : shifted_node_id for shifted_node_id, node_id in enumerate(current_graph_node_ids)}\n",
    "    mapped_edges = [(node_ids_map[row], node_ids_map[col]) for row, col in current_graph_edges]\n",
    "\n",
    "    # Add edge list to graph\n",
    "    g.add_edge_list(mapped_edges)\n",
    "        \n",
    "    # Compute combination n choose 3 (n is the number of nodes in current graph)\n",
    "    n_choose_three = comb(g.num_vertices(), 3)\n",
    "\n",
    "    # Compute and store clustering coefficient \n",
    "    clustering_coefficient = gt.global_clustering(g, ret_counts = True)\n",
    "    \n",
    "    if n_choose_three > 0:\n",
    "        clustering_coefficients.append(clustering_coefficient[1] / n_choose_three)\n",
    "    else:\n",
    "        clustering_coefficients.append(clustering_coefficient[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering coefficients have been written in datasets/ENZYMES/ENZYMES.global_cc successfully\n"
     ]
    }
   ],
   "source": [
    "# Write global clustering coefficient of all graphs in a text file\n",
    "output_file =  os.path.join(datasets_folder, dataset_name, f\"{dataset_name}.global_cc\")\n",
    "\n",
    "with open(output_file, \"w\") as file:\n",
    "    for clustering_coefficient in clustering_coefficients:\n",
    "        file.write(f\"{clustering_coefficient}\\n\")\n",
    "\n",
    "print(f\"Clustering coefficients have been written in {output_file} successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
