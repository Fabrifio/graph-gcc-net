{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define names of datasets to select\n",
    "dataset_target_name_list = [\"ENZYMES\", \"DD\"]\n",
    "\n",
    "# Define datasets destination\n",
    "datasets_folder = \"datasets/\"\n",
    "\n",
    "# List of all datasets names\n",
    "dataset_name_list = os.listdir(datasets_folder)\n",
    "\n",
    "# List of dataset files and label files\n",
    "dataset_file_list = []\n",
    "dataset_label_file_list = []\n",
    "\n",
    "# File paths of dataset edge indexes and labels\n",
    "for dataset_name in dataset_name_list:\n",
    "    if(dataset_name in dataset_target_name_list):\n",
    "        dataset_file_list.append(os.path.join(datasets_folder, dataset_name, f\"{dataset_name}.pth\"))\n",
    "        dataset_label_file_list.append(os.path.join(datasets_folder, dataset_name, f\"{dataset_name}.global_cc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable gpu for training, validation and test if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of datasets\n",
    "dataset_list = []\n",
    "\n",
    "# List of dataset labels list\n",
    "dataset_labels_list = []\n",
    "\n",
    "for dataset_file, dataset_label_file in zip(dataset_file_list, dataset_label_file_list):\n",
    "    # Load dataset\n",
    "    dataset = torch.load(dataset_file, weights_only=True)\n",
    "\n",
    "    # Load labels from file .global_cc\n",
    "    with open(dataset_label_file, 'r') as f:\n",
    "        dataset_labels = [float(line.strip()) for line in f.readlines()]\n",
    "\n",
    "    # Prepare dataset list to manage datasets easily\n",
    "    dataset_list.append(dataset)\n",
    "\n",
    "    # Prepare dataset labels\n",
    "    dataset_labels_list.append(dataset_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structure for storing Data object of graphs\n",
    "data_list = []\n",
    "\n",
    "# Create pytorch geometric Data objects from graphs\n",
    "for dataset, dataset_labels in zip(dataset_list, dataset_labels_list):\n",
    "    for i, (key, tensor) in enumerate(dataset.items()):\n",
    "        # Set edge index, node features and label of the current graph\n",
    "        edge_index = tensor.coalesce().indices()\n",
    "        num_nodes = edge_index.max().item() + 1\n",
    "        x = degree(edge_index[0], num_nodes, dtype=torch.float) / num_nodes\n",
    "        x = x.view(-1, 1)\n",
    "        label = torch.tensor([dataset_labels[i]])\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index, y=label)\n",
    "        data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader split in train, validation and test\n",
    "total_len = len(data_list)\n",
    "train_len = int(0.8 * total_len)\n",
    "validation_len = int(0.1 * total_len)\n",
    "test_len = total_len - train_len - validation_len\n",
    "\n",
    "train_data, validation_data, test_data = random_split(data_list, [train_len, validation_len, test_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader structures for train, valdation and test\n",
    "train_data_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "validation_data_loader = DataLoader(validation_data, batch_size=32, shuffle=True)\n",
    "test_data_loader = DataLoader(test_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as func\n",
    "from torch.nn import Sequential, Linear, ReLU, ModuleList\n",
    "from torch_geometric.nn import GINConv, BatchNorm, TopKPooling, global_mean_pool, dense_diff_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN model definition\n",
    "class ClusteringCoefficientGNN(torch.nn.Module):\n",
    "    # Model architecture\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, gin_layers, dropout):\n",
    "        super(ClusteringCoefficientGNN, self).__init__()\n",
    "\n",
    "        # Initialize module lists\n",
    "        self.convs = ModuleList()\n",
    "        self.bns = ModuleList()\n",
    "        \n",
    "        # Input GIN layer\n",
    "        self.convs.append(GINConv(\n",
    "            Sequential(\n",
    "                Linear(in_channels, hidden_channels[0]),\n",
    "                ReLU(),\n",
    "                Linear(hidden_channels[0], hidden_channels[0])\n",
    "            ), train_eps=self.training\n",
    "        ))\n",
    "        self.bns.append(BatchNorm(hidden_channels[0]))\n",
    "        \n",
    "        # GIN layers (hidden layers)\n",
    "        for i in range(gin_layers - 1):\n",
    "            self.convs.append(GINConv(\n",
    "                Sequential(\n",
    "                    Linear(hidden_channels[0] if i == 0 else hidden_channels[1], hidden_channels[1]),\n",
    "                    ReLU(),\n",
    "                    Linear(hidden_channels[1], hidden_channels[1])\n",
    "                ), train_eps=self.training \n",
    "            ))\n",
    "            self.bns.append(BatchNorm(hidden_channels[1]))\n",
    "        \n",
    "        # Output fully connected layer\n",
    "        self.linear = Sequential(\n",
    "            Linear(hidden_channels[1], hidden_channels[1]),\n",
    "            ReLU(),\n",
    "            Linear(hidden_channels[1], out_channels)\n",
    "        )\n",
    "        \n",
    "        self.dropout = dropout\n",
    "\n",
    "    # Forward pass (inference)\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # GIN layers\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            x = conv(x, edge_index)\n",
    "            x = bn(x)\n",
    "            x = func.relu(x)\n",
    "            x = func.dropout(x, p = self.dropout, training = self.training)\n",
    "\n",
    "        # Global mean pool (graph-level features)\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # Fully connected layer\n",
    "        x = self.linear(x)\n",
    "        x = func.relu(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, train_data_loader, optimizer, loss_function):\n",
    "    # Set GNN model to training mode\n",
    "    model.train()\n",
    "    train_total_loss = 0\n",
    "\n",
    "    # Train over all graphs in training_data_loader\n",
    "    for data in train_data_loader:\n",
    "        # Clear the gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move data to device (gpu if available)\n",
    "        data = data.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(out.squeeze(dim=1), data.y)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_total_loss += loss.item()\n",
    "\n",
    "    return train_total_loss / len(train_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function (for validation and test)\n",
    "@torch.no_grad()\n",
    "def evaluate(model, evaluation_data_loader, loss_function):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    evaluation_total_loss = 0\n",
    "\n",
    "    # Evaluation over all graphs in evaluation_data_loader\n",
    "    for data in evaluation_data_loader:\n",
    "        # Move data to device (gpu if available)\n",
    "        data = data.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(out.squeeze(dim=1), data.y)\n",
    "\n",
    "        evaluation_total_loss += loss.item()\n",
    "\n",
    "    return evaluation_total_loss / len(evaluation_data_loader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of node features\n",
    "in_channels = 1\n",
    "\n",
    "# Output dimension (regression task over scalar numbers)\n",
    "out_channels = 1\n",
    "\n",
    "# Hyperparameters for GNN\n",
    "hidden_channels = [64, 32]\n",
    "\n",
    "# Number of layers\n",
    "gin_layers = 4\n",
    "\n",
    "# Training settings\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "patience = 1000\n",
    "\n",
    "# Dropout probability\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = ClusteringCoefficientGNN(in_channels, hidden_channels, out_channels, gin_layers, dropout)\n",
    "model.to(device)\n",
    "\n",
    "# Initialize optimizer and loss function\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "loss_function = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Train Loss: 0.0018, Val Loss: 0.0007\n",
      "Epoch 200, Train Loss: 0.0018, Val Loss: 0.0005\n",
      "Epoch 300, Train Loss: 0.0021, Val Loss: 0.0005\n",
      "Epoch 400, Train Loss: 0.0018, Val Loss: 0.0007\n",
      "Epoch 500, Train Loss: 0.0020, Val Loss: 0.0005\n",
      "Epoch 600, Train Loss: 0.0017, Val Loss: 0.0004\n",
      "Epoch 700, Train Loss: 0.0018, Val Loss: 0.0006\n",
      "Epoch 800, Train Loss: 0.0018, Val Loss: 0.0008\n",
      "Epoch 900, Train Loss: 0.0017, Val Loss: 0.0004\n",
      "Epoch 1000, Train Loss: 0.0017, Val Loss: 0.0005\n",
      "Test Loss: 0.0055\n"
     ]
    }
   ],
   "source": [
    "# Number of epochs the GNN has not obtained better validation loss\n",
    "patience_counter = 0\n",
    "\n",
    "# Best validation loss up to now\n",
    "best_validation_loss = float('inf')\n",
    "\n",
    "# Training and validation loop\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Training step\n",
    "    train_loss = train(model, train_data_loader, optimizer, loss_function)\n",
    "\n",
    "    # Validation step\n",
    "    val_loss = evaluate(model, validation_data_loader, loss_function)\n",
    "\n",
    "    # Training status in current epoch\n",
    "    if(epoch % 100 == 0):\n",
    "        print(f'Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    # Early stopping condition using patience\n",
    "    if(val_loss < best_validation_loss):\n",
    "        best_validation_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if(patience_counter == patience):\n",
    "            print(\"Patience finished: stopping training\")\n",
    "            break\n",
    "\n",
    "# Final test phase\n",
    "test_loss = evaluate(model, test_data_loader, loss_function)\n",
    "print(f'Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save location in 'model/'\n",
    "model_folder = \"model/\"\n",
    "os.makedirs(model_folder, exist_ok = True)\n",
    "\n",
    "# Save GNN model\n",
    "torch.save(model.state_dict(), os.path.join(model_folder, 'graph_gcc_net.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='cuda:0')\n",
      "tensor([[0.]], device='cuda:0', grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "index = 10\n",
    "data_point = data_list[index].to(device)\n",
    "out = model(data_point.x, data_point.edge_index, data.batch)\n",
    "print(data_list[index].y)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
